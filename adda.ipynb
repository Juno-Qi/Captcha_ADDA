{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GRU, TimeDistributed\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, RepeatVector,Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.optimizers import SGD\n",
    "import imageio\n",
    "import glob\n",
    "import random\n",
    "import PIL\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "import keras\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars 52\n"
     ]
    }
   ],
   "source": [
    "characters =  list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "#nums = [str(li) for li in range(1,10)]  # 1-9\n",
    "nums = []\n",
    "all_chars = characters + nums\n",
    "char_index = {}\n",
    "for (idx,ch) in enumerate(all_chars):\n",
    "    char_index[ch] = idx\n",
    "print \"total chars\", len(char_index)\n",
    "\n",
    "H, W, C = 60, 60, 3\n",
    "def generate_data(font, total_samples, output_dir, digits_num=3, width=100, height=100, font_sizes=[50]):\n",
    "    image = ImageCaptcha(fonts=[font], width=width, height=height, font_sizes=font_sizes)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)    \n",
    "    labels = []\n",
    "    total_images = []\n",
    "    for i in range(total_samples):\n",
    "        cur_cap = sample(characters, digits_num)\n",
    "        cur_cap =''.join(cur_cap)\n",
    "        image.write(cur_cap, output_dir + \"images/\" + str(i) +\".png\")\n",
    "        # resize it and convert to gray scale\n",
    "        #img = Image.open(output_dir + \"images/\" + str(i) +\".png\").convert('L')\n",
    "        img = Image.open(output_dir + \"images/\" + str(i) +\".png\")\n",
    "        img = img.resize((width, height))\n",
    "        \n",
    "        img.save(output_dir + \"images/\" + str(i) +\".png\")\n",
    "        img = imageio.imread(output_dir + \"images/\" + str(i) +\".png\")\n",
    "        total_images.append(np.array(img))\n",
    "        \n",
    "        label = []\n",
    "        for ch in cur_cap:\n",
    "            label.append(char_index[ch])\n",
    "        labels.append(label)\n",
    "    labels = np.asarray(labels)\n",
    "    labels.dump(output_dir + \"labels/\" + \"labels\")\n",
    "    images = np.asarray(total_images)\n",
    "    images.dump(output_dir + \"images/\" + \"images\")\n",
    "    \n",
    "# generate source image        \n",
    "#generate_data(\"fonts/font_source/LeagueGothic-Regular.otf\", 20000, \"sourceImages/\")\n",
    "    \n",
    "# generate target image\n",
    "#generate_data(\"fonts/font_target/OpenSans-Bold.ttf\", 15000, \"targetImages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXwHYIwMsJvM6vAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1407db0390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check source sample\n",
    "img = mpimg.imread('sourceImages/images/9.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbxglxG58wyghduMbRgn5bx7k/J2btqGKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14059eff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#check target sample\n",
    "img = mpimg.imread('targetImages/images/9.png')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape (20000, 100, 100, 3)\n",
      "input labels shape (20000, 3, 52)\n",
      "images shape (15000, 100, 100, 3)\n",
      "input labels shape (15000, 3, 52)\n"
     ]
    }
   ],
   "source": [
    "# create a function to load all data\n",
    "def load_data(images_dir, labels_dir, split_ratio):\n",
    "    \n",
    "    with open(images_dir, \"rb\")as f:\n",
    "        images = np.load(f)\n",
    "\n",
    "    images = np.asarray(images)\n",
    "    \n",
    "    with open(labels_dir, \"rb\") as f:\n",
    "        labels = np.load(f)\n",
    "\n",
    "    vocab_size = len(all_chars)\n",
    "\n",
    "    labels_categorical = np.asarray([to_categorical(label, vocab_size) for label in labels])\n",
    "    print \"images shape\", images.shape\n",
    "    # print images[0]\n",
    "    print \"input labels shape\", labels_categorical.shape\n",
    "    total = images.shape[0]\n",
    "    seed = range(total)\n",
    "    random.shuffle(seed)\n",
    "    split_index = int(total*split_ratio)\n",
    "    train_data = images[seed[0:split_index]]\n",
    "    train_label = labels_categorical[seed[0:split_index]]\n",
    "    val_data = images[seed[split_index:]]\n",
    "    val_label = labels_categorical[seed[split_index:]]\n",
    "    return (train_data, train_label, val_data, val_label)\n",
    "\n",
    "source_trainX, source_trainY, source_valX, source_valY = load_data(\"sourceImages/images/images\",\"sourceImages/labels/labels\", 0.75)\n",
    "target_trainX, target_trainY, _, _ = load_data(\"targetImages/images/images\",\"targetImages/labels/labels\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_classifier(vocab_size, num_outputs_chars = 3, image_shape=(100,100,3)):\n",
    "    '''\n",
    "    num_outputs_chars: how many output should the model have; we only have one output by default, which means the sequence only has three letters\n",
    "    '''\n",
    "    image_model = Sequential()\n",
    "    image_model.add(Convolution2D(32, (3, 3), padding='valid', input_shape=image_shape))\n",
    "    image_model.add(BatchNormalization())\n",
    "    image_model.add(Activation('relu'))\n",
    "    image_model.add(Convolution2D(32, (3, 3), padding='valid'))\n",
    "    image_model.add(BatchNormalization())\n",
    "    image_model.add(Activation('relu'))\n",
    "    image_model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "    image_model.add(Dropout(0.25))\n",
    "    image_model.add(Convolution2D(64, (3, 3), padding='valid'))\n",
    "    image_model.add(BatchNormalization())\n",
    "    image_model.add(Activation('relu'))\n",
    "    image_model.add(Convolution2D(64, (3, 3),padding='valid'))\n",
    "    image_model.add(BatchNormalization())\n",
    "    image_model.add(Activation('relu'))\n",
    "    image_model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "    image_model.add(Dropout(0.25))\n",
    "    image_model.add(Flatten())\n",
    "    \n",
    "    image_model.summary()\n",
    "    #for layer in image_model.layers:\n",
    "    #    print(layer.get_output_at(0).get_shape().as_list())\n",
    "    # Note: Keras does automatic shape inference.\n",
    "    image_input = Input(shape=image_shape)\n",
    "    encoded_image = image_model(image_input)\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(num_outputs_chars):\n",
    "        out1 = Dense(128, activation=\"relu\")(encoded_image)\n",
    "        output1 = Dense(vocab_size, activation=\"softmax\")(out1)\n",
    "        outputs.append(output1)\n",
    "    model = Model([image_input], outputs)\n",
    "    return (model,encoded_image, image_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_371 (Conv2D)          (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_370 (Bat (None, 98, 98, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_370 (Activation)  (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_372 (Conv2D)          (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_371 (Bat (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_371 (Activation)  (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_185 (MaxPoolin (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_163 (Dropout)        (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_373 (Conv2D)          (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_372 (Bat (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_372 (Activation)  (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_374 (Conv2D)          (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_373 (Bat (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_373 (Activation)  (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_186 (MaxPoolin (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_93 (Flatten)         (None, 30976)             0         \n",
      "=================================================================\n",
      "Total params: 66,336\n",
      "Trainable params: 65,952\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "source_classifier summary\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_143 (InputLayer)           (None, 100, 100, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_107 (Sequential)      (None, 30976)         66336       input_143[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_612 (Dense)                (None, 128)           3965056     sequential_107[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_614 (Dense)                (None, 128)           3965056     sequential_107[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_616 (Dense)                (None, 128)           3965056     sequential_107[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_613 (Dense)                (None, 52)            6708        dense_612[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_615 (Dense)                (None, 52)            6708        dense_614[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_617 (Dense)                (None, 52)            6708        dense_616[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 11,981,628\n",
      "Trainable params: 11,981,244\n",
      "Non-trainable params: 384\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "source_classifier,_,_= create_classifier(len(all_chars))\n",
    "print(\"source_classifier summary\")\n",
    "source_classifier.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "source_classifier.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "checkpointer= ModelCheckpoint(filepath=\"sourceModel/weights.{epoch:02d}.hdf5\")\n",
    "# we only use one letter \n",
    "digit1 = source_trainY[:,0,:]\n",
    "digit2 = source_trainY[:,1,:]\n",
    "digit3 = source_trainY[:,2,:]\n",
    "\n",
    "val_digit1 = source_valY[:,0,:]\n",
    "val_digit2 = source_valY[:,1,:]\n",
    "val_digit3 = source_valY[:,2,:]\n",
    "\n",
    "source_classifier = load_model(\"sourceModel/source_model.hdf5\")\n",
    "\n",
    "source_classifier.fit(source_trainX, [digit1, digit2, digit3], validation_data = (source_valX,[val_digit1, val_digit2, val_digit3]), shuffle=True,batch_size=64, epochs=40,callbacks=[checkpointer])\n",
    "source_classifier.save(\"sourceModel/source_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14976/15000 [============================>.] - ETA: 0s[41.00128176066081, 13.186067669677735, 13.313342899576822, 14.501871175130209, 0.040266666666666666, 0.023, 0.021933333333333332]\n"
     ]
    }
   ],
   "source": [
    "def predict_on_dataset(model, data, label):\n",
    "    return model.evaluate(x=data, y=label)\n",
    "    \n",
    "# let's see the accuracy of the source classifier on target dataset\n",
    "target_digit1 = target_trainY[:,0,:]\n",
    "target_digit2 = target_trainY[:,1,:]\n",
    "target_digit3 = target_trainY[:,2,:]\n",
    "# from the above result we can know that the validataion result is best when epoch = 2\n",
    "source_classifier=load_model(\"sourceModel/weights.01.hdf5\")\n",
    "print predict_on_dataset(source_classifier, data=target_trainX, label=[target_digit1, target_digit2, target_digit3])\n",
    "# from the result we can notice that the model predict a bad result on target data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/slynedeng/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., name=\"discriminator\", inputs=Tensor(\"in...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_152 (InputLayer)       (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_654 (Dense)            (None, 100)               3097700   \n",
      "_________________________________________________________________\n",
      "dense_655 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_656 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 3,108,002\n",
      "Trainable params: 3,108,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_391 (Conv2D)          (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_390 (Bat (None, 98, 98, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_390 (Activation)  (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_392 (Conv2D)          (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_391 (Bat (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_391 (Activation)  (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_195 (MaxPoolin (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_393 (Conv2D)          (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_392 (Bat (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_392 (Activation)  (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_394 (Conv2D)          (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_393 (Bat (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_393 (Activation)  (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_196 (MaxPoolin (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_98 (Flatten)         (None, 30976)             0         \n",
      "=================================================================\n",
      "Total params: 66,336\n",
      "Trainable params: 65,952\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "target model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_153 (InputLayer)       (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "sequential_143 (Sequential)  (None, 30976)             66336     \n",
      "=================================================================\n",
      "Total params: 66,336\n",
      "Trainable params: 65,952\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "combined model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_153 (InputLayer)       (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "target_model (Model)         (None, 30976)             66336     \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 2)                 3108002   \n",
      "=================================================================\n",
      "Total params: 3,174,338\n",
      "Trainable params: 65,952\n",
      "Non-trainable params: 3,108,386\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def domain_classifier(encoded_image_tensor):\n",
    "    x = Dense(100, activation='relu')(encoded_image_tensor)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(2, activation = 'sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "source_or_target_tensor = Input(shape=(22*22*64,))\n",
    "discriminator_model = Model(inputs=source_or_target_tensor, output=domain_classifier(source_or_target_tensor), name=\"discriminator\")\n",
    "print(\"discriminator model summary\")\n",
    "discriminator_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "discriminator_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "discriminator_model.trainable=False\n",
    "\n",
    "\n",
    "_,target_image_tensor,image_input = create_classifier(len(all_chars))\n",
    "#print(keras.backend.shape(target_image_tensor))  # input shape should be inferred from here\n",
    "target_model = Model(inputs = image_input, outputs=target_image_tensor, name=\"target_model\")\n",
    "print(\"target model summary\")\n",
    "target_model.summary()\n",
    "\n",
    "temp = target_model(image_input)\n",
    "combined_model = Model(inputs=image_input, outputs=discriminator_model(temp))\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "combined_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(\"combined model summary\")\n",
    "combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_101 (InputLayer)           (None, 100, 100, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_144 (Sequential)      (None, 30976)         66336       input_101[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_447 (Dense)                (None, 128)           3965056     sequential_144[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_449 (Dense)                (None, 128)           3965056     sequential_144[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_451 (Dense)                (None, 128)           3965056     sequential_144[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_448 (Dense)                (None, 52)            6708        dense_447[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_450 (Dense)                (None, 52)            6708        dense_449[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_452 (Dense)                (None, 52)            6708        dense_451[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 11,981,628\n",
      "Trainable params: 0\n",
      "Non-trainable params: 11,981,628\n",
      "____________________________________________________________________________________________________\n",
      "Tensor(\"sequential_144/flatten_74/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "source_classifier = load_model(\"sourceModel/weights.01.hdf5\")\n",
    "for layer in source_classifier.layers:\n",
    "    layer.trainable = False\n",
    "source_classifier.summary()\n",
    "\n",
    "print(source_classifier.layers[1].get_output_at(1))\n",
    "target_layer = 1\n",
    "get_source_encode_layer_output = K.function([source_classifier.layers[0].input, K.learning_phase()],\n",
    "                                  [source_classifier.layers[1].get_output_at(1)])\n",
    "\n",
    "# output in test mode = 0\n",
    "# output in train mode = 1\n",
    "#source_classifier_tensor = get_source_encode_layer_output([x,0])[0]\n",
    "\n",
    "#source_classifier_tensor = intermediate_layer_model.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to pre train\n",
      "[ 0.15035945  0.97421875]\n",
      "[ 0.04450023  0.99453127]\n",
      "[ 0.02521302  0.99781251]\n",
      "[ 0.01872749  0.99921876]\n",
      "[ 0.01624485  0.99937499]\n",
      "[ 0.01316375  0.99984378]\n",
      "[ 0.01151829  0.99953127]\n",
      "[ 0.01043863  0.99984378]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-555-17d7f33790bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msample_target_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_target_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtarget_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_target_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss_fake\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_target_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fake\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/slynedeng/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/slynedeng/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/slynedeng/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/slynedeng/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0muninitialized_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# begin to train alternatively\n",
    "def data_generator(X_train, y_train, batch_size):\n",
    "    idx = 0\n",
    "    total = len(X_train)\n",
    "    while 1:\n",
    "        p = np.random.permutation(len(X_train)) # shuffle each time\n",
    "        X_train = X_train[p]\n",
    "        y_train = y_train[p]\n",
    "        for i in range(total/batch_size):\n",
    "            yield X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "        \n",
    "\n",
    "source_data_generator = data_generator(source_trainX, source_trainY, 32)\n",
    "target_data_generator = data_generator(target_trainX, target_trainY, 32)\n",
    "\n",
    "\n",
    "loss_fake = np.zeros(shape=len(combined_model.metrics_names))\n",
    "#pretrain\n",
    "\n",
    "print(\"begin to pre train\")\n",
    "for i in range(4000):\n",
    "        sample_target_x, sample_target_y = next(target_data_generator)\n",
    "        target_y = to_categorical(np.ones(len(sample_target_y)),num_classes=2)\n",
    "        loss_fake= np.add(combined_model.train_on_batch(sample_target_x, target_y), loss_fake)\n",
    "        if i % 200 == 0 and i > 0:\n",
    "            print(loss_fake/200)\n",
    "            loss_fake = 0\n",
    "print(\"finish pretrain\")\n",
    "\n",
    "\n",
    "total_training_steps = 15000/32 * 1\n",
    "k_d = 1\n",
    "k_g = 2\n",
    "loss_fake = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "loss_dis = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "print(discriminator_model.metrics_names)\n",
    "\n",
    "for t in range(total_training_steps):    \n",
    "    for i in range(k_g):\n",
    "        sample_target_x, sample_target_y = next(target_data_generator)\n",
    "        target_y = to_categorical(np.ones(len(sample_target_y)),num_classes=2)\n",
    "        sample_target_x2, sample_target_y = next(target_data_generator)\n",
    "        target_y2 = to_categorical(np.ones(len(sample_target_y)),num_classes=2)\n",
    "        combine_x = np.concatenate((sample_target_x, sample_target_x2),axis = 0)\n",
    "        combine_y = np.concatenate((target_y, target_y2), axis = 0)\n",
    "        loss_fake= np.add(combined_model.train_on_batch(combine_x, combine_y), loss_fake)\n",
    "        \n",
    "        \n",
    "    for i in range(k_d):\n",
    "        sample_source_x, sample_source_y = next(source_data_generator)\n",
    "        sample_target_x, sample_target_y = next(target_data_generator)\n",
    "        source_y = to_categorical(np.ones(len(sample_source_y)), num_classes=2)\n",
    "        target_y = to_categorical(np.zeros(len(sample_target_y)), num_classes=2)\n",
    "        source_tensor_output = get_source_encode_layer_output([sample_source_x,0])[0]\n",
    "        target_tensor_output = target_model.predict(sample_target_x)\n",
    "        combine_source_target = np.concatenate((source_tensor_output,target_tensor_output), axis = 0)\n",
    "        combine_y = np.concatenate((source_y, target_y), axis = 0)\n",
    "        loss_dis = np.add(discriminator_model.train_on_batch(combine_source_target, combine_y),loss_dis)\n",
    "\n",
    "                 \n",
    "    if (t % 10) == 0:\n",
    "        print \"loss fake\", loss_fake/(10*k_g)\n",
    "        print \"loss_dis\", loss_dis/(10*k_d)\n",
    "\n",
    "        loss_fake = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        loss_dis= np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        \n",
    "target_model.save(\"targetModel/target_model.hdf5\")\n",
    "discriminator_model.save(\"discriminatorModel/discriminator_model.hdf5\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 30976)\n"
     ]
    }
   ],
   "source": [
    "source_classifier = load_model(\"sourceModel/weights.01.hdf5\")\n",
    "data=target_trainX\n",
    "label=[target_digit1, target_digit2, target_digit3]\n",
    "target_encoding = target_model.predict(data)\n",
    "print(target_encoding.shape)\n",
    "\n",
    "#print(source_classifier.layers)\n",
    "model = source_classifier\n",
    "end_classify = K.function([model.layers[2].input,K.learning_phase()],\n",
    "                                  [model.layers[-3].output, model.layers[-2].output, model.layers[-1].output])\n",
    "\n",
    "label1_predict= []\n",
    "label2_predict = []\n",
    "label3_predict = []\n",
    "for i in range(len(target_encoding)):\n",
    "    layer_output1, layer_output2, layer_output3 = end_classify([[target_encoding[i]],0])\n",
    "    label1_predict.append(np.argmax(layer_output1))\n",
    "    label2_predict.append(np.argmax(layer_output2))\n",
    "    label3_predict.append(np.argmax(layer_output3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 15, 51, 12, 18, 38, 17, 4, 45, 44, 45, 36, 17, 12, 18, 45, 12, 18, 15, 15]\n"
     ]
    }
   ],
   "source": [
    "print(label1_predict[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label1 = np.argmax(target_digit1,axis=1)\n",
    "true_label2 = np.argmax(target_digit2, axis=1)\n",
    "true_label3 = np.argmax(target_digit3, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0367333333333\n",
      "0.062\n",
      "0.0183333333333\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(label1_predict == true_label1)/float(len(label1_predict)))\n",
    "print(np.sum(label2_predict == true_label1)/float(len(label2_predict)))\n",
    "print(np.sum(label3_predict == true_label1)/float(len(label3_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
